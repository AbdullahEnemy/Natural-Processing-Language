{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d1d173f2-c6d0-434b-b9c8-20fa56c8f216",
   "metadata": {},
   "source": [
    "Name:Abdullah Fayyaz\n",
    "RollNumber:21l-5208\n",
    "Section:8A\n",
    "Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b397746c-764e-4010-983e-4fdd55babf32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mpprint\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m request\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk, re, pprint\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from math import log, exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7f5ab-3b8c-498d-99d1-59d317f19f83",
   "metadata": {},
   "source": [
    "Question NO 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "226a14ba-a4f0-48d1-88c1-8467d6abd003",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Description\n",
    "a. [a-zA-Z]+\n",
    "This matches one or more uppercase (A-Z) or lowercase (a-z) letters.\n",
    "Example:Hello\", \"abc\", \"Python\"\n",
    "b. [A-Z][a-z]*\n",
    "This matches an uppercase letter (A-Z) followed by zero or more lowercase letters (a-z).\n",
    "Example:\"Hello\", \"Python\", \"A\"\n",
    "c.p[aeiou]{,2}t\n",
    "This matches the letter 'p', followed by 0 to 2 vowels (aeiou), followed by the letter 't'.\n",
    "Example:\"pt\", \"pat\", \"pet\", \"pout\"\n",
    "d. \\d+(\\.\\d+)?\n",
    "This matches one or more digits (\\d+), optionally followed by a decimal point and more digits ((\\.\\d+)?).\n",
    "Example: \"123\", \"45.67\", \"0.5\"\n",
    "([^aeiou][aeiou][^aeiou])*\n",
    "This matches any number of sequences where:\n",
    "The first character is not a vowel ([^aeiou]).\n",
    "The second character is a vowel ([aeiou]).\n",
    "The third character is not a vowel ([^aeiou]).\n",
    "Example:\"bat\", \"cat\", \"big\", \"tap\"\n",
    "f. \\w+|[^\\w\\s]+\n",
    "This matches either:\n",
    "\\w+: One or more word characters ([a-zA-Z0-9_]).\n",
    "[^\\w\\s]+: One or more non-word and non-space characters (i.e., punctuation and special characters).\n",
    "Example:\"hello@\", \"world\", \"123\", \"!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae55f539-510a-4839-af8d-8c2b2c7516f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing pattern a: [a-zA-Z]+\n",
      "{Hello} {Pat}, {my} {cat} {is} 3.14 {feet} {away}!\n",
      "\n",
      "Testing pattern b: [A-Z][a-z]*\n",
      "{Hello} {Pat}, my cat is 3.14 feet away!\n",
      "\n",
      "Testing pattern c: p[aeiou]{,2}t\n",
      "Hello Pat, my cat is 3.14 feet away!\n",
      "\n",
      "Testing pattern d: \\d+(\\.\\d+)?\n",
      "Hello Pat, my cat is {3.14} feet away!\n",
      "\n",
      "Testing pattern e: ([^aeiou][aeiou][^aeiou])*\n",
      "{Hello Pat}{},{} {}m{}y{} {cat is}{} {}3{}.{}1{}4{} {}f{}e{}e{}t{ aw}{}a{}y{}!{}\n",
      "\n",
      "Testing pattern f: \\w+|[^\\w\\s]+\n",
      "{Hello} {Pat}{,} {my} {cat} {is} {3}{.}{14} {feet} {away}{!}\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello Pat, my cat is 3.14 feet away!\"\n",
    "# Define regex patterns\n",
    "patterns = {\n",
    "    \"a\": r\"[a-zA-Z]+\",\n",
    "    \"b\": r\"[A-Z][a-z]*\",\n",
    "    \"c\": r\"p[aeiou]{,2}t\",\n",
    "    \"d\": r\"\\d+(\\.\\d+)?\",\n",
    "    \"e\": r\"([^aeiou][aeiou][^aeiou])*\",\n",
    "    \"f\": r\"\\w+|[^\\w\\s]+\"\n",
    "}\n",
    "\n",
    "for key, pattern in patterns.items():\n",
    "    print(f\"\\nTesting pattern {key}: {pattern}\")\n",
    "    nltk.re_show(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017d011-3f02-4774-a415-aa83d8d7b6d6",
   "metadata": {},
   "source": [
    "Question No 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeeb92ce-d2a4-4d85-bffd-ab62e9cdbbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Determiner Matches:\n",
      "a: Match\n",
      "an: Match\n",
      "the: Match\n",
      "apple: No Match\n",
      "theatre: No Match\n",
      "----------------------------------------------\n",
      "Arithmetic Expression Matches:\n",
      "2+3: Match\n",
      "4*5+6: Match\n",
      "12 * 7 + 5: Match\n",
      "2++3: No Match\n",
      "4*/5: No Match\n",
      "5+*6: No Match\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Regix Patterns\n",
    "determiner_pattern = r\"^(a|an|the)$\"\n",
    "arithmetic_pattern = r\"^\\d+(\\s*[\\+\\*]\\s*\\d+)*$\"\n",
    "#Texts to Check\n",
    "determiners = [\"a\", \"an\", \"the\", \"apple\", \"theatre\"]\n",
    "expressions = [\"2+3\", \"4*5+6\", \"12 * 7 + 5\", \"2++3\", \"4*/5\", \"5+*6\"]\n",
    "# Testing\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Determiner Matches:\")\n",
    "for word in determiners:\n",
    "    print(f\"{word}: {'Match' if re.fullmatch(determiner_pattern, word) else 'No Match'}\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Arithmetic Expression Matches:\")\n",
    "for expr in expressions:\n",
    "    print(f\"{expr}: {'Match' if re.fullmatch(arithmetic_pattern, expr) else 'No Match'}\")\n",
    "print(\"----------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04169e-58db-41db-a90c-3c6d9b772ccf",
   "metadata": {},
   "source": [
    "Question No 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c021253e-a0ad-40f7-a873-7ee23d5a46a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contact Us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "Admissions\n",
      "\n",
      "\n",
      "Offered Programs\n",
      "\n",
      "\n",
      "Admission Schedule\n",
      "\n",
      "\n",
      "How To Apply\n",
      "\n",
      "\n",
      "Eligibility Criteria\n",
      "\n",
      "\n",
      "Loans/Scholarships\n",
      "\n",
      "\n",
      "Test Pattern\n",
      "\n",
      "\n",
      "Fee Structure\n",
      "\n",
      "\n",
      "Prospectus 2024\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Campuses\n",
      "\n",
      "\n",
      "Chiniot-Faisalabad Campus\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "Faculty & Staff\n",
      "\n",
      "\n",
      "PhD Supervisors\n",
      "\n",
      "\n",
      "Events\n",
      "\n",
      "\n",
      "Medal Holders\n",
      "\n",
      "\n",
      "Rector's List\n",
      "\n",
      "\n",
      "Dean's List\n",
      "\n",
      "\n",
      "Scholarship Awardees\n",
      "\n",
      "\n",
      "EE Department (Mission Vision)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Islamabad Campus\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "Faculty & Staff\n",
      "\n",
      "\n",
      "PhD Supervisors\n",
      "\n",
      "\n",
      "Events\n",
      "\n",
      "\n",
      "Research\n",
      "\n",
      "\n",
      "Medal Holders\n",
      "\n",
      "\n",
      "Rector's List\n",
      "\n",
      "\n",
      "Dean's List\n",
      "\n",
      "\n",
      "Scholarship Awardees\n",
      "\n",
      "\n",
      "EE Department (Mission Vision)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Karachi Campus\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "Faculty & Staff\n",
      "\n",
      "\n",
      "PhD Supervisors\n",
      "\n",
      "\n",
      "Events\n",
      "\n",
      "\n",
      "Research\n",
      "\n",
      "\n",
      "Medal Holders\n",
      "\n",
      "\n",
      "Rector's List\n",
      "\n",
      "\n",
      "Dean's List\n",
      "\n",
      "\n",
      "Scholarship Awardees\n",
      "\n",
      "\n",
      "EE Department (Mission Vision)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lahore Campus\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "Faculty & Staff\n",
      "\n",
      "\n",
      "PhD Supervisors\n",
      "\n",
      "\n",
      "Events\n",
      "\n",
      "\n",
      "Research\n",
      "\n",
      "\n",
      "Medal Holders\n",
      "\n",
      "\n",
      "Rector's List\n",
      "\n",
      "\n",
      "Dean's List\n",
      "\n",
      "\n",
      "Scholarship Awardees\n",
      "\n",
      "\n",
      "Civil Engg. Department (Mission Vision)\n",
      "\n",
      "\n",
      "EE Depa\n"
     ]
    }
   ],
   "source": [
    "def fetch_text_from_url(url):\n",
    "    try:\n",
    "        # Fetching Content from the url\n",
    "        response = request.urlopen(url)\n",
    "        html = response.read().decode('utf8')\n",
    "        # Extracting text from prased html\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching URL: {e}\"\n",
    "# Example usage\n",
    "url = \"https://www.nu.edu.pk/home/ContactUs\"\n",
    "clean_text = fetch_text_from_url(url)\n",
    "print(clean_text[:1000])#1000 means first 1000 chracters are pritnted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b1fb7-576f-4512-bf3c-f6d3aadcaac5",
   "metadata": {},
   "source": [
    "Question no 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "658fdda0-074c-4941-86d4-8e7ff2f80b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Email Addresses:\n",
      "admissions.cfd@nu.edu.pk\n",
      "admissions.isb@nu.edu.pk\n",
      "admissions.khi@nu.edu.pk\n",
      "admissions.lhr@nu.edu.pk\n",
      "admissions.pwr@nu.edu.pk\n",
      "\n",
      "Extracted Phone Numbers:\n",
      "051) 831 4100\n",
      "051) 410 0619\n",
      "042) 516 5680\n",
      "021) 343 9094\n",
      "021) 341 0054\n",
      "021) 341 0054\n",
      "091) 582 2320\n",
      "041) 260 7272\n",
      "051) 2855072\n"
     ]
    }
   ],
   "source": [
    "def extract_contacts(text):\n",
    "    # Regular expression for email addresses\n",
    "    email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    \n",
    "    # Regular expression for phone numbers (various formats)\n",
    "    phone_pattern = r\"\\+?\\d{1,3}[-.\\s]?\\(?\\d{2,4}\\)?[-.\\s]?\\d{3,4}[-.\\s]?\\d{4}\"\n",
    "\n",
    "    # Find all matches\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    phones = re.findall(phone_pattern, text)\n",
    "\n",
    "    return emails, phones\n",
    "\n",
    "\n",
    "# Extract emails and phone numbers\n",
    "emails, phones = extract_contacts(clean_text)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nExtracted Email Addresses:\")\n",
    "print(\"\\n\".join(emails) if emails else \"No emails found.\")\n",
    "\n",
    "print(\"\\nExtracted Phone Numbers:\")\n",
    "print(\"\\n\".join(phones) if phones else \"No phone numbers found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32817f6a-1162-4da5-a36b-5710c39d4f0c",
   "metadata": {},
   "source": [
    "Question no 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2083163-54ae-4039-a687-0e694005f2d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens:    ['running', 'runner', 'easily', 'studies', 'studying', 'flies', 'happiness']\n",
      "Porter Stemmed:     ['run', 'runner', 'easili', 'studi', 'studi', 'fli', 'happi']\n",
      "Lancaster Stemmed:  ['run', 'run', 'easy', 'study', 'study', 'fli', 'happy']\n"
     ]
    }
   ],
   "source": [
    "text = \"running runner easily studies studying flies happiness\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "porter_stems = [porter.stem(word) for word in tokens]\n",
    "lancaster_stems = [lancaster.stem(word) for word in tokens]\n",
    "\n",
    "# Print results\n",
    "print(\"Original Tokens:   \", tokens)\n",
    "print(\"Porter Stemmed:    \", porter_stems)\n",
    "print(\"Lancaster Stemmed: \", lancaster_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826607c-862a-4f0b-a254-ac9dfa04e150",
   "metadata": {},
   "source": [
    "Question no 5\n",
    "- **First word:** Starts with 'Z' or 'z', followed by lowercase letters, ends with 'a'.  \n",
    "- **Second word:** Starts with a digit, 'k', lowercase letters, ends with a digit.  \n",
    "- **Third word:** Starts with 'c', has '*', ends with 'a'.  \n",
    "- **Fourth word:** Starts with 'P' or 'p', two letters, ends with 'a'.  \n",
    "- **Fifth word:** *Dae* (based on all four words, as it forms the phrase \"Zalima Coca Cola Pila Dae\").  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a20edefb-5877-4766-8d86-1c30e3df7523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Message: Zalima 1kacokaco1 cola Pila Dae\n"
     ]
    }
   ],
   "source": [
    "def extract_hidden_message(text):\n",
    "\n",
    "    first_word_pattern = r'\\bZ[a-z]*a\\b|\\bz[a-z]*a\\b'\n",
    "    first_word = re.findall(first_word_pattern, text)\n",
    "    second_word_pattern = r'\\b\\d[k][a-z]*\\d\\b'\n",
    "    second_word = re.findall(second_word_pattern, text)\n",
    "    second_word_actual = second_word[0] if second_word else \"\"\n",
    "    \n",
    "    third_word_pattern = r'\\bc[a-z]*\\*[a-z]+a\\b'\n",
    "    third_word = re.findall(third_word_pattern, text)\n",
    "    third_word_actual = third_word[0].replace('*', '') if third_word else \"\"\n",
    "    \n",
    "\n",
    "    fourth_word_pattern = r'\\b[Pp][a-z]{2}a\\b'\n",
    "    fourth_word = re.findall(fourth_word_pattern, text)\n",
    "    \n",
    "    \n",
    "    fifth_word = \"Dae\"\n",
    "\n",
    "    message = f\"{first_word[0]} {second_word_actual} {third_word_actual} {fourth_word[0]} {fifth_word}\"\n",
    "    return message\n",
    "\n",
    "text = \"\"\"Pila Forfeited you engrossed but 1kometimes explained. Another 1kacokaco1 as studied it to evident. \n",
    "Merry sense 9given he be arisepila. Conduct at an replied removal an amongst.\n",
    "Remainingzalima 0determine few her two cordially Zalima admitting old. Sometimes ctra*nger his \n",
    "pisdsdla ourselves her co*la depending you boy. Eat discretion cultivated possession far comparison \n",
    "projection pila considered. And few fat interested discovered inquietude insensible unsatiable increasing \n",
    "zalima eat.\"\"\"\n",
    "\n",
    "hidden_message = extract_hidden_message(text)\n",
    "print(\"Hidden Message:\", hidden_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b652a2ad-9b24-4260-9e51-5b819027b393",
   "metadata": {},
   "source": [
    "PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1b8e2e89-3a2d-4660-920d-b566659a30e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Unigram Model output:\n",
      "Generated Sentence: <s> , have film high don't . , , . way rest combat he start on this performance grip <s> who american necessity man what of copy , is <s> , plot , . . got ) in of so a in see anyone large club , only methodical his a although journey <s> bits : led end damon . affair the : flight actors of action it uses nsa . fact his back two made-for-tv would his has </s>\n",
      "Probability of \"picture\": 1.4700327127270493e-05\n",
      "\n",
      "Sample Smoothed Unigram Model output:\n",
      "Generated Sentence: <s> , , . most the it's escapist of and going , bleibtreu managed takes he \" </s>\n",
      "Probability of \"picture\": 1.4039430061156616e-05\n",
      "\n",
      "Sample Bigram Model output:\n",
      "Generated Sentence: <s> conformity and instead , but . </s>\n",
      "Probability of \"picture\": 0.0\n",
      "\n",
      "Sample Smoothed Bigram Model output:\n",
      "Generated Sentence: <s> the time thief who were superfluous UNK constructed with two leads to make it is the scientists must stress this movie like \" footloose \" but it is simon birch , aiming at night . </s>\n",
      "Probability of \"picture\": 4.780215880708568e-06\n",
      "\n",
      "Sample Unigram Distribution output:\n",
      "Probability of \"picture\": 0.0003967776842611519\n",
      "Random Draw: mib\n",
      "\n",
      "Computing Perplexity...\n",
      "Unigram Model Perplexity (pos test): 512.1673281305104\n",
      "Unigram Model Perplexity (neg test): 494.2369381866578\n",
      "Bigram Model Perplexity (pos test): 33.87806072195685\n",
      "Bigram Model Perplexity (neg test): 32.05276137877031\n",
      "Smoothed Unigram Model Perplexity (pos test): 513.9473165955059\n",
      "Smoothed Unigram Model Perplexity (neg test): 496.17527806666516\n",
      "Smoothed Bigram Model Perplexity (pos test): 399.29398695933014\n",
      "Smoothed Bigram Model Perplexity (neg test): 397.3003270450333\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "# Constants \n",
    "UNK = \"UNK\"     # Unknown word token\n",
    "start = \"<s>\"   # Start-of-sentence token\n",
    "end = \"</s>\"    # End-of-sentence-token\n",
    "\n",
    "# Read a text file into a corpus\n",
    "def readFileToCorpus(f):\n",
    "    if os.path.isfile(f):\n",
    "        with open(f, \"r\") as file:\n",
    "            corpus = [line.split() for line in file]\n",
    "        return corpus\n",
    "    else:\n",
    "        print(\"Error: corpus file\", f, \"does not exist\")\n",
    "        sys.exit()\n",
    "\n",
    "# Preprocess the corpus\n",
    "def preprocess(corpus):\n",
    "    freqDict = defaultdict(int)\n",
    "    for sen in corpus:\n",
    "        for word in sen:\n",
    "            freqDict[word] += 1\n",
    "\n",
    "    for sen in corpus:\n",
    "        for i in range(len(sen)):\n",
    "            if freqDict[sen[i]] < 2:\n",
    "                sen[i] = UNK\n",
    "\n",
    "    for sen in corpus:\n",
    "        sen.insert(0, start)\n",
    "        sen.append(end)\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "def preprocessTest(vocab, corpus):\n",
    "    for sen in corpus:\n",
    "        for i in range(len(sen)):\n",
    "            if sen[i] not in vocab:\n",
    "                sen[i] = UNK\n",
    "\n",
    "    for sen in corpus:\n",
    "        sen.insert(0, start)\n",
    "        sen.append(end)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "# Unigram Model\n",
    "class UnigramModel:\n",
    "    def __init__(self, corpus):\n",
    "        self.counts = defaultdict(int)\n",
    "        self.total = 0\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                self.counts[word] += 1\n",
    "                self.total += 1\n",
    "    \n",
    "    def generateSentence(self):\n",
    "        sentence = [start]\n",
    "        while True:\n",
    "            word = random.choices(list(self.counts.keys()), weights=self.counts.values())[0]\n",
    "            sentence.append(word)\n",
    "            if word == end:\n",
    "                break\n",
    "        return sentence\n",
    "    \n",
    "    def getSentenceProbability(self, sen):\n",
    "        prob = 1.0\n",
    "        for word in sen[1:]:\n",
    "            prob *= self.counts[word] / self.total\n",
    "        return prob\n",
    "\n",
    "# Smoothed Unigram Model\n",
    "class SmoothedUnigramModel(UnigramModel):\n",
    "    def __init__(self, corpus):\n",
    "        super().__init__(corpus)\n",
    "        self.vocab_size = len(self.counts)\n",
    "    \n",
    "    def getSentenceProbability(self, sen):\n",
    "        prob = 1.0\n",
    "        for word in sen[1:]:\n",
    "            prob *= (self.counts[word] + 1) / (self.total + self.vocab_size)\n",
    "        return prob\n",
    "\n",
    "# Bigram Model\n",
    "class BigramModel:\n",
    "    def __init__(self, corpus):\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        for sentence in corpus:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.unigram_counts[sentence[i]] += 1\n",
    "                self.bigram_counts[sentence[i]][sentence[i+1]] += 1\n",
    "    \n",
    "    def generateSentence(self):\n",
    "        sentence = [start]\n",
    "        while True:\n",
    "            word = random.choices(list(self.bigram_counts[sentence[-1]].keys()), \n",
    "                                  weights=self.bigram_counts[sentence[-1]].values())[0]\n",
    "            sentence.append(word)\n",
    "            if word == end:\n",
    "                break\n",
    "        return sentence\n",
    "    \n",
    "    def getSentenceProbability(self, sen):\n",
    "        prob = 1.0\n",
    "        for i in range(len(sen) - 1):\n",
    "            prob *= self.bigram_counts[sen[i]][sen[i+1]] / self.unigram_counts[sen[i]]\n",
    "        return prob\n",
    "\n",
    "# Smoothed Bigram Model\n",
    "class SmoothedBigramModel(BigramModel):\n",
    "    def __init__(self, corpus, lambda1=0.5, lambda2=0.5):\n",
    "        super().__init__(corpus)\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.total = sum(self.unigram_counts.values())\n",
    "        self.vocab_size = len(self.unigram_counts)\n",
    "    \n",
    "    def getSentenceProbability(self, sen):\n",
    "        prob = 1.0\n",
    "        for i in range(len(sen) - 1):\n",
    "            unigram_prob = (self.unigram_counts[sen[i]] + 1) / (self.total + self.vocab_size)\n",
    "            bigram_prob = (self.bigram_counts[sen[i]][sen[i+1]] + 1) / (self.unigram_counts[sen[i]] + self.vocab_size)\n",
    "            prob *= self.lambda1 * bigram_prob + self.lambda2 * unigram_prob\n",
    "        return prob\n",
    "\n",
    "# Unigram Distribution\n",
    "class UnigramDist:\n",
    "    def __init__(self, corpus):\n",
    "        self.counts = defaultdict(float)\n",
    "        self.total = 0.0\n",
    "        self.train(corpus)\n",
    "\n",
    "    def train(self, corpus):\n",
    "        for sen in corpus:\n",
    "            for word in sen:\n",
    "                if word == start:\n",
    "                    continue\n",
    "                self.counts[word] += 1.0\n",
    "                self.total += 1.0\n",
    "\n",
    "    def prob(self, word):\n",
    "        return self.counts[word] / self.total if self.total > 0 else 0\n",
    "\n",
    "    def draw(self):\n",
    "        rand = random.random()\n",
    "        for word in self.counts.keys():\n",
    "            rand -= self.prob(word)\n",
    "            if rand <= 0.0:\n",
    "                return word\n",
    "def computePerplexity(model, corpus):\n",
    "    log_prob_sum = 0.0\n",
    "    word_count = 0\n",
    "    for sentence in corpus:\n",
    "        prob = model.getSentenceProbability(sentence)\n",
    "        if prob > 0:\n",
    "            log_prob_sum += -math.log(prob)\n",
    "            word_count += len(sentence)\n",
    "    return math.exp(log_prob_sum / word_count) if word_count > 0 else float('inf')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainCorpus = readFileToCorpus('train.txt')\n",
    "    trainCorpus = preprocess(trainCorpus)\n",
    "    \n",
    "    unigram_model = UnigramModel(trainCorpus)\n",
    "    bigram_model = BigramModel(trainCorpus)\n",
    "    smoothed_unigram_model = SmoothedUnigramModel(trainCorpus)\n",
    "    smoothed_bigram_model = SmoothedBigramModel(trainCorpus)\n",
    "    unigram_dist = UnigramDist(trainCorpus)\n",
    "    \n",
    "    print(\"Sample Unigram Model output:\")\n",
    "    print(\"Generated Sentence:\", \" \".join(unigram_model.generateSentence()))\n",
    "    print(\"Probability of \\\"picture\\\":\", unigram_model.getSentenceProbability([\"<s>\", \"picture\", \"</s>\"]))\n",
    "    \n",
    "    print(\"\\nSample Smoothed Unigram Model output:\")\n",
    "    print(\"Generated Sentence:\", \" \".join(smoothed_unigram_model.generateSentence()))\n",
    "    print(\"Probability of \\\"picture\\\":\", smoothed_unigram_model.getSentenceProbability([\"<s>\", \"picture\", \"</s>\"]))\n",
    "    \n",
    "    print(\"\\nSample Bigram Model output:\")\n",
    "    print(\"Generated Sentence:\", \" \".join(bigram_model.generateSentence()))\n",
    "    print(\"Probability of \\\"picture\\\":\", bigram_model.getSentenceProbability([\"<s>\", \"picture\", \"</s>\"]))\n",
    "    \n",
    "    print(\"\\nSample Smoothed Bigram Model output:\")\n",
    "    print(\"Generated Sentence:\", \" \".join(smoothed_bigram_model.generateSentence()))\n",
    "    print(\"Probability of \\\"picture\\\":\", smoothed_bigram_model.getSentenceProbability([\"<s>\", \"picture\", \"</s>\"]))\n",
    "    \n",
    "    print(\"\\nSample Unigram Distribution output:\")\n",
    "    print(\"Probability of \\\"picture\\\":\", unigram_dist.prob(\"picture\"))\n",
    "    print(\"Random Draw:\", unigram_dist.draw())\n",
    "    print(\"\\nComputing Perplexity...\")\n",
    "    print(\"Unigram Model Perplexity (pos test):\", computePerplexity(unigram_model, posTestCorpus))\n",
    "    print(\"Unigram Model Perplexity (neg test):\", computePerplexity(unigram_model, negTestCorpus))\n",
    "    print(\"Bigram Model Perplexity (pos test):\", computePerplexity(bigram_model, posTestCorpus))\n",
    "    print(\"Bigram Model Perplexity (neg test):\", computePerplexity(bigram_model, negTestCorpus))\n",
    "    print(\"Smoothed Unigram Model Perplexity (pos test):\", computePerplexity(smoothed_unigram_model, posTestCorpus))\n",
    "    print(\"Smoothed Unigram Model Perplexity (neg test):\", computePerplexity(smoothed_unigram_model, negTestCorpus))\n",
    "    print(\"Smoothed Bigram Model Perplexity (pos test):\", computePerplexity(smoothed_bigram_model, posTestCorpus))\n",
    "    print(\"Smoothed Bigram Model Perplexity (neg test):\", computePerplexity(smoothed_bigram_model, negTestCorpus))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22df63a2-2a4d-4e31-9314-4a3908783f5c",
   "metadata": {},
   "source": [
    "Sentence Length Control:\n",
    "\n",
    "The Unigram Model generates words independently, and sentence length depends on the probability of </s> appearing.\n",
    "The Bigram Model chooses words based on previous words, leading to more structured sentence lengths.\n",
    "Probability Differences:\n",
    "\n",
    "The Unigram Model assigns smoother probabilities since it ignores word order.\n",
    "The Bigram Model assigns lower probabilities to unseen word pairs, making probability differences more extreme.\n",
    "Better Sentence Generation:\n",
    "\n",
    "The Smoothed Bigram Model generates more natural sentences since it handles unseen word pairs better than the basic Bigram Model.\n",
    "Perplexity Comparison:\n",
    "\n",
    "Negative test corpus has higher perplexity across all models because it likely contains more diverse or less predictable sentences.\n",
    "Smoothed Bigram Model has the lowest perplexity, meaning it generalizes best.\n",
    "Let me know if you need any refinements! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3568d-1342-4afb-8cb9-c7c305dd84c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1918282-c92e-4d2a-af31-a375c78ff57e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
